{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "LFnT2MWe1IPx"
      },
      "outputs": [],
      "source": [
        "##### DOWNLOAD DATASET #####\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle # installing the kaggle package\n",
        "!mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n",
        "!cp kaggle.json ~/.kaggle/ # move the key to the folder\n",
        "!pwd # checking the present working directory\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d gsimonx37/letterboxd\n",
        "!unzip /content/letterboxd.zip -d /content/letterbox/\n",
        "clear_output()\n",
        "############################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### SPARK CONTEXT #####################\n",
        "import pandas as pd\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "\n",
        "sc = spark.sparkContext\n",
        "###############################################"
      ],
      "metadata": {
        "id": "FPDSHZ_ydWnr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## BASKET CREATION ################\n",
        "#TODO do it using SPARK directly -> cars = spark.read.csv('cars.csv', header=True, sep=\";\")\n",
        "actors = pd.read_csv(\"letterbox/actors.csv\")\n",
        "baskets = actors.groupby(\"id\")[\"name\"].apply(list)\n",
        "\n",
        "print(\"number of baskets: \" + str(len(baskets)))\n",
        "print(\"biggest basket: \" + str(baskets.map(len).max()))\n",
        "print(baskets)\n",
        "baskets_RDD = sc.parallelize(baskets).cache()\n",
        "##############################################"
      ],
      "metadata": {
        "id": "7S860HiwhKp1",
        "outputId": "5c3e79d2-5e43-4b5a-8a14-de6a412df925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of baskets: 603163\n",
            "biggest basket: 451\n",
            "id\n",
            "1000001    [Margot Robbie, Lewis Easter, Onyemachi Ejimof...\n",
            "1000002    [Rosie Peralta, Anna Elisabeth Rihlmann, Andre...\n",
            "1000003    [Randall Archer, Boon Pin Koh, Efka Kvaracieju...\n",
            "1000004    [Leonard Termo, Greg Bronson, Michael Arturo, ...\n",
            "1000005    [Lena Georgas, Jeff Hephner, Elyes Gabel, Broo...\n",
            "                                 ...                        \n",
            "1896377    [Marat Basharov, Sergey Rost, Gleb Kalyuzhny, ...\n",
            "1896382                                   [Marine Petrosyan]\n",
            "1896387                                        [Rebecca Jim]\n",
            "1896391                                       [Ko Shibasaki]\n",
            "1896393                                    [Atsuhiro Inukai]\n",
            "Name: name, Length: 603163, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baskets_RDD.getNumPartitions()"
      ],
      "metadata": {
        "id": "a7yTSr78dKw2",
        "outputId": "bbad82cd-9d25-464c-8cbf-31f74ac2cb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## APRIORI\n",
        "def apriori(chunk, s):\n",
        "\n",
        "  n_bsk = 0 # number of baskets in chunk used to calculate support\n",
        "  i = 0\n",
        "  trsl = dict()\n",
        "\n",
        "  def translate(e, dic):\n",
        "    try:\n",
        "      return dic[e]\n",
        "    except KeyError:\n",
        "      dic[e] = len(dic)+1\n",
        "      return dic[e]\n",
        "\n",
        "\n",
        "  for bsk in chunk:\n",
        "    n_bsk += 1;\n",
        "    translated = [translate(x,trsl) for x in bsk]\n",
        "\n",
        "\n",
        "\n",
        "  def first_pass():\n",
        "    pass\n",
        "\n",
        "  def second_pass():\n",
        "    pass\n",
        "\n",
        "\n",
        "  yield (len(trsl),1)"
      ],
      "metadata": {
        "id": "RieQuIkAo7c_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def first_red(key,value):\n",
        "  yield key\n",
        "def second_map(candidate, chunk):\n",
        "  yield (1,1)\n",
        "def second_red(key,value):\n",
        "  yield key\n",
        "\n",
        "chunk = 1; #TMP to remove\n",
        "s = 10; #support threshold\n",
        "\n",
        "baskets_RDD.mapPartitions(lambda x: apriori(x,s)).reduceByKey(lambda a,b: a).collect()\n"
      ],
      "metadata": {
        "id": "NMhukWkCZLND",
        "outputId": "74d80a45-6b19-4e03-9f06-dd49f8124609",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1039348, 1), (788811, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSa3ABzYCu9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}